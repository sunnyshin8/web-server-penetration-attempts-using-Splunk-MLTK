# Web Server Penetration Attempt Detection using Splunk MLTK

## Overview
This project focuses on detecting web server penetration attempts using Splunk's Machine Learning Toolkit (MLTK) and the Common Information Model (CIM). The solution analyzes Apache web server logs to identify suspicious behavior patterns that indicate potential security threats.

## Project Structure
```
├── access.log                       # Original Apache log file (137,511 entries)
├── access_parsed.csv                # Basic parsed CSV data for analysis
├── access_parsed_enhanced.csv       # Enhanced CSV with security indicators & derived fields
├── parse_apache_log.py              # Basic log parsing script
├── parse_apache_log_enhanced.py     # Enhanced parser with security analysis
├── data_analysis/                   # Data analysis and preprocessing scripts
├── ml_models/                       # Machine learning models and configurations
├── splunk_configs/                  # Splunk configuration files
├── dashboards/                      # Splunk dashboard configurations
├── alerts/                          # Alert configurations and rules
├── documentation/                   # Project documentation
├── requirements.txt                 # Python dependencies
├── setup.bat / setup.sh             # Automated setup scripts
└── README.md                        # This file
```

## Key Features

### 1. CIM-Aligned Log Ingestion
- Standardize web logs by mapping fields to Splunk's Web data model
- Fields mapped: `clientip`, `http_method`, `uri_path`, `status`, `user_agent`
- Consistent analysis across different log formats

### 2. Unsupervised Anomaly Detection
- Utilize Splunk MLTK's unsupervised algorithms
- Establish baseline behavior patterns
- Surface anomalies indicating potential penetration attempts

### 3. Attack Behavior Recognition
- **Failed Authentication Attempts**: Repeated 401/403 responses from same IP
- **Directory/URL Fuzzing**: Multiple 404 responses indicating scanning
- **Injection Attempts**: Long, encoded, or suspicious URL paths
- **Brute Force Detection**: High-frequency access attempts on sensitive endpoints

### 4. Interactive Dashboards
- IP-based activity trends visualization
- Most targeted endpoints analysis
- Failed access attempt patterns
- Real-time threat monitoring

### 5. Alerting Mechanism
- Real-time alerts for critical threats
- Scheduled reports for ongoing monitoring
- Threshold-based detection rules

## Data Sources

### Available CSV Files

#### 1. Basic Parsed Data (`access_parsed.csv`)
- **Size**: 137,510 entries from Apache access logs
- **Fields**: Standard Apache log fields (IP, datetime, method, path, status, size, referrer, user_agent)
- **Purpose**: Basic analysis and Splunk ingestion
- **Generated by**: `parse_apache_log.py`

#### 2. Enhanced Security Data (`access_parsed_enhanced.csv`)
- **Size**: Same log entries with 35+ additional fields
- **Security Indicators**: Automated detection of attack patterns including:
  - SQL injection attempts
  - XSS (Cross-site scripting) patterns
  - Directory traversal attempts
  - Command injection indicators
  - Admin panel access attempts
  - Suspicious file access
  - Bot/crawler identification
- **Derived Fields**: Path analysis, time-based features, response categorization
- **Purpose**: Advanced security analysis and ML training
- **Generated by**: `parse_apache_log_enhanced.py`

### Dataset Statistics
- **Time Period**: November 30, 2021 (24-hour period)
- **Total Requests**: 137,511 log entries
- **Unique IP Addresses**: ~15,000+ unique sources
- **Geographic Coverage**: Global traffic patterns
- **Attack Indicators Found**:
  - Failed authentication attempts (401/403 responses)
  - Directory enumeration attempts (404 patterns)
  - Bot/crawler traffic identification
  - WordPress admin access attempts
  - File extension probing
  - Various HTTP methods (GET, HEAD, POST)

### Field Comparison

| Category | Basic CSV | Enhanced CSV | Description |
|----------|-----------|--------------|-------------|
| **Core Fields** | 8 fields | 8 fields | IP, datetime, method, path, status, size, referrer, user_agent |
| **Security Flags** | 0 | 8 fields | has_sql_injection, has_xss, has_directory_traversal, etc. |
| **Derived Analysis** | 0 | 19 fields | path_length, status_category, time_analysis, etc. |
| **Total Fields** | 8 | 35+ | Comprehensive security-focused dataset |

## Data Model

### Log Fields (CIM Compliant)
- `clientip` (src_ip): Source IP address
- `http_method`: HTTP request method (GET, POST, etc.)
- `uri_path`: Requested URL path
- `status`: HTTP response status code
- `bytes_in`: Request size
- `bytes_out`: Response size
- `user_agent`: Client user agent string
- `referer`: HTTP referer header
- `_time`: Timestamp of the request

### Derived Fields
- `is_suspicious_path`: Boolean flag for suspicious URLs
- `is_failed_auth`: Boolean flag for authentication failures
- `request_frequency`: Requests per minute per IP
- `path_length`: URL path character count
- `has_special_chars`: Boolean flag for special characters in path

### Which CSV Should You Use?

#### Use `access_parsed.csv` when:
- ✅ New to Splunk or security analysis
- ✅ Need standard web log analysis
- ✅ Working with limited Splunk resources
- ✅ Prefer to build custom security logic in Splunk
- ✅ Need smaller file size for testing

#### Use `access_parsed_enhanced.csv` when:
- ✅ Focused on security analysis and threat detection
- ✅ Want pre-computed security indicators
- ✅ Planning to use machine learning models
- ✅ Need comprehensive attack pattern analysis
- ✅ Want to skip manual field extraction in Splunk
- ✅ Building advanced security dashboards

#### File Size Comparison
- **Basic CSV**: ~25 MB (8 fields per record)
- **Enhanced CSV**: ~75 MB (35+ fields per record)
- **Trade-off**: 3x larger file size for 4x more analytical value

## Installation and Setup

### 1. Prerequisites
- Splunk Enterprise or Splunk Cloud
- Machine Learning Toolkit (MLTK) app installed
- Python 3.7+ for data preprocessing
- Access to web server logs

### 2. Data Ingestion
1. Choose your preferred CSV file (basic or enhanced)
2. Upload the selected CSV to Splunk or configure log ingestion
3. Apply the provided transforms and props configurations
4. Verify data is properly mapped to Web CIM model

### 3. Model Training
1. Run the baseline behavior analysis
2. Train anomaly detection models using historical data
3. Set appropriate thresholds based on environment

### 4. Dashboard Deployment
1. Import dashboard configurations
2. Customize visualizations for your environment
3. Set up user permissions and access controls

### 5. Alert Configuration
1. Deploy alert rules for critical threats
2. Configure notification channels (email, Slack, etc.)
3. Test alert functionality with known attack patterns

## Quick Start

### Option 1: Basic Analysis (Recommended for Splunk beginners)
```bash
# Parse logs with basic fields
python parse_apache_log.py

# Upload access_parsed.csv to Splunk
# Use standard Splunk searches for analysis
```

### Option 2: Advanced Security Analysis (Recommended for security teams)
```bash
# Parse logs with security indicators
python parse_apache_log_enhanced.py access.log -o access_parsed_enhanced.csv --stats

# Use the enhanced CSV for ML training and advanced analysis
python data_analysis/web_log_analyzer.py
python ml_models/security_ml_models.py
```

### Option 3: Automated Setup
```bash
# Windows
setup.bat

# Linux/Mac
chmod +x setup.sh && ./setup.sh
```

## Usage Examples

### Working with Basic CSV (access_parsed.csv)
```spl
# Basic threat detection using standard fields
index=webserver sourcetype=csv_access_logs
| eval is_suspicious = if(status IN ("401", "403", "404"), 1, 0)
| stats count by ip, is_suspicious
| where count > 10 AND is_suspicious = 1
```

### Working with Enhanced CSV (access_parsed_enhanced.csv)
```spl
# Advanced threat detection using security indicators
index=webserver sourcetype=csv_access_logs
| where has_sql_injection=1 OR has_xss=1 OR has_directory_traversal=1
| stats count by ip, path, has_sql_injection, has_xss, has_directory_traversal
| sort -count
```

### Combined Analysis
```spl
# Leverage derived fields for sophisticated analysis
index=webserver sourcetype=csv_access_logs
| where is_error=1 AND path_length>100 AND (has_special_chars=1 OR is_bot=0)
| eval threat_score = if(has_sql_injection=1, 10, 0) + 
                     if(has_directory_traversal=1, 8, 0) + 
                     if(has_admin_access=1, 6, 0)
| where threat_score > 5
| stats sum(threat_score) as total_threat_score, count by ip
| sort -total_threat_score
```

### Basic Threat Detection
```spl
index=webserver sourcetype=access_combined
| eval is_suspicious = if(status IN ("401", "403", "404"), 1, 0)
| stats count by clientip, is_suspicious
| where count > 10 AND is_suspicious = 1
```

### Anomaly Detection with MLTK
```spl
index=webserver sourcetype=access_combined
| bucket _time span=1h
| stats count by _time, clientip
| apply isolation_forest
| where outlier_score > 0.7
```

### SQL Injection Detection
```spl
index=webserver sourcetype=access_combined
| regex uri_path="(?i)(union|select|insert|update|delete|drop|create)"
| stats count by clientip, uri_path
| sort -count
```

## Attack Scenarios Detected

1. **Brute Force Attacks**: Multiple failed login attempts
2. **Directory Traversal**: Attempts to access unauthorized directories
3. **SQL Injection**: Malicious SQL code in URL parameters
4. **Cross-Site Scripting (XSS)**: Script injection attempts
5. **Web Scanning**: Automated vulnerability scanning tools
6. **Bot Traffic**: Malicious bot detection and filtering

## Performance Considerations

- **Data Volume**: Optimized for high-volume log processing
- **Real-time Processing**: Stream processing for immediate threat detection
- **Storage**: Efficient data retention policies
- **Scalability**: Horizontal scaling support for large deployments

## Known Limitations

1. **Low-and-Slow Attacks**: May bypass frequency-based detection
2. **Sophisticated Evasion**: Advanced attackers may mimic legitimate traffic
3. **False Positives**: Internal testing may trigger alerts
4. **Data Quality**: Detection accuracy depends on complete log data

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Support

For questions and support:
- Create an issue in the repository
- Check the documentation folder for detailed guides
- Review Splunk MLTK documentation for advanced configurations

## Version History

- **v1.0.0**: Initial release with basic anomaly detection
- **v1.1.0**: Added advanced attack pattern recognition
- **v1.2.0**: Enhanced dashboards and alerting capabilities
